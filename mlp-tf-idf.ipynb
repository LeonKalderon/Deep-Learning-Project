{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www2.aueb.gr/users/ion/docs/koutsikakis_msc_thesis.pdf\n",
    "<br>\n",
    "https://github.com/anmolchawla/Kaggle-Toxic-Comment-Classification-Challenge\n",
    "<br>\n",
    "https://github.com/sumitgouthaman/toxic-comment-classification\n",
    "<br>\n",
    "Multiclass\n",
    "https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/\n",
    "<br>\n",
    "GloveEmb: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "<br>\n",
    "15th solution: https://github.com/imrahulr/Toxic-Comment-Classification-Kaggle/blob/master/attention_merged/attention_300d.ipynb\n",
    "<br>\n",
    "12th solution, self-attention and other: https://github.com/thinline72/toxic\n",
    "\n",
    "self-attention: https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n",
    "<br>\n",
    "simple LSTM: https://www.kaggle.com/thousandvoices/simple-lstm\n",
    "\n",
    "Cross-Validation keras\n",
    "https://www.kaggle.com/stefanie04736/simple-keras-model-with-k-fold-cross-validation\n",
    "https://www.kaggle.com/franklemuchahary/basic-cnn-keras-with-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepreplay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-edec5f850849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparabola\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepreplay'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from deepreplay.callbacks import ReplayData\n",
    "from deepreplay.datasets.parabola import load_data\n",
    "\n",
    "from keras import backend as K # Importing Keras backend (by default it is Tensorflow)\n",
    "from keras.layers import Input, Dense, Dropout # Layers to be used for building our model\n",
    "from keras.models import Model # The class used to create a model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils # Utilities to manipulate numpy arrays\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau\n",
    "from keras.wrappers.scikit_learn import KerasClassifier #Wrapper for scikit\n",
    "from tensorflow import set_random_seed # Used for reproducible experiments\n",
    "import keras \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, learning_curve, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Import clean data\n",
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "datasets_path = '../input/'\n",
    "train = pd.read_csv(datasets_path + 'cleaned_train.csv').fillna(' ')\n",
    "test = pd.read_csv(datasets_path + 'cleaned_test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "y_train = train[classes].values\n",
    "y_test = pd.read_csv(datasets_path + 'test_labels.csv')\n",
    "y_test = y_test[classes].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e78592edb5dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m word_vectorizer = TfidfVectorizer(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(3, 4),\n",
    "    max_features=2000)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA on TF-IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TruncatedSVD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8c7c37dce27c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4321\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_features_svd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_features_svd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TruncatedSVD' is not defined"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=4321)\n",
    "train_features_svd = svd.fit_transform(train_features)\n",
    "test_features_svd = svd.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Custom Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ae5dc43db335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Define a class for model evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRocAucEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/yekenot/pooled-gru-fasttext\n",
    "\n",
    "#Define a class for model evaluation\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, training_data=(),validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_tra, self.y_tra = training_data\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.aucs_val = []\n",
    "        self.aucs_tra = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):                   \n",
    "        y_pred_val = self.model.predict(self.X_val, verbose=0)\n",
    "        score_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "\n",
    "        y_pred_tra = self.model.predict(self.X_tra, verbose=0)\n",
    "        score_tra = roc_auc_score(self.y_tra, y_pred_tra)\n",
    "\n",
    "        self.aucs_tra.append(score_val)\n",
    "        self.aucs_val.append(score_tra)\n",
    "        print(\"\\n ROC-AUC - epoch: %d - score_tra: %.6f - score_val: %.6f \\n\" % (epoch+1, score_tra, score_val))\n",
    "\n",
    "def recall(y_true, y_pred):    \n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):    \n",
    "    \"\"\"\n",
    "    Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    Source\n",
    "    ------\n",
    "    https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"Calculate the F1 score.\"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r))\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Evaluation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plots:\n",
    "    def plot_history(history):\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        x = range(1, len(val_loss) + 1)\n",
    "\n",
    "        plt.plot(x, loss, 'b', label='Training loss')\n",
    "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "\n",
    "    def plot_roc_auc(train_roc, val_roc):\n",
    "        x = range(1, len(val_roc) + 1)\n",
    "\n",
    "        plt.plot(x, train_roc, 'b', label='Training RocAuc')\n",
    "        plt.plot(x, val_roc, 'r', label='Validation RocAuc')\n",
    "        plt.title('Training and validation RocAuc')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(\n",
    "    input_size,\n",
    "    optimizer,    \n",
    "    classes=6,  \n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    hidden_layers=1,\n",
    "    units=600,\n",
    "    dropout_rate=0.8,\n",
    "    l2_lambda=0.0,\n",
    "    batch_norm=False,\n",
    "    funnel=False,\n",
    "    hidden_activation='relu',\n",
    "    output_activation='sigmoid'\n",
    "):\n",
    "  \n",
    "    # Define the seed for numpy and Tensorflow to have reproducible experiments.\n",
    "    np.random.seed(1402) \n",
    "    set_random_seed(1981)\n",
    "       \n",
    "    input = Input(\n",
    "        shape=(input_size,),\n",
    "        name='Input'\n",
    "    )\n",
    "    x = input\n",
    "    print(x.shape)\n",
    "    # Define the hidden layers.\n",
    "    for i in range(hidden_layers):\n",
    "        if funnel:\n",
    "            layer_units=units // (i+1)\n",
    "        else: \n",
    "            layer_units=units\n",
    "        x = Dense(\n",
    "           units=layer_units,\n",
    "           kernel_initializer='glorot_uniform',\n",
    "           kernel_regularizer=l2(l2_lambda),\n",
    "           activation=hidden_activation,\n",
    "           name='Hidden-{0:d}'.format(i + 1)\n",
    "        )(x)\n",
    "        #Dropout\n",
    "        if dropout_rate != 0:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "    # Define the output layer.    \n",
    "    output = Dense(\n",
    "        units=classes,\n",
    "        kernel_initializer='uniform',\n",
    "        activation=output_activation,\n",
    "        name='Output'\n",
    "    )(x)\n",
    "    # Define the model and train it.\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "      \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f66fa1ef86a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mRocAuc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRocAucEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(train_features, y_train, train_size=0.90, random_state=3)\n",
    "RocAuc = RocAucEvaluation(training_data=(X_tra, y_tra) ,validation_data=(X_val, y_val))\n",
    "\n",
    "batch_size = 256\n",
    "num_classes = 6\n",
    "epochs = 1\n",
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "model = MLP_model(\n",
    "    input_size = X_tra.shape[1],\n",
    "    optimizer = optimizer,    \n",
    "    classes=num_classes,  \n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    hidden_layers=2,\n",
    "    units=1000,\n",
    "    dropout_rate=0.5,\n",
    "    l2_lambda=0.0,\n",
    "    batch_norm=False,\n",
    "    funnel=True,\n",
    "    hidden_activation='relu',\n",
    "    output_activation='sigmoid'\n",
    ")\n",
    "\n",
    "# Keras Callbacks\n",
    "reducer_lr = ReduceLROnPlateau(factor = 0.00002, patience = 1, min_lr = 1e-6, verbose = 1)\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', mode='min', patience = 10) # Change 4 to 8 in the final run\n",
    "model_file_name = 'weights_base.best.hdf5'\n",
    "check_pointer = keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_binary_crossentropy', mode='min', verbose = 1, save_best_only = True)  \n",
    "log_file_name = 'model.log'\n",
    "csv_logger = keras.callbacks.CSVLogger(log_file_name)\n",
    "# replaydata = ReplayData(X_tra, y_tra, filename='hyperparms_in_action.h5', group_name='part1')\n",
    "callbacks_list = [early_stopper, check_pointer, csv_logger, RocAuc, reducer_lr]\n",
    "\n",
    "model.fit(x=X_tra,\n",
    "          y=y_tra,          \n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=epochs,\n",
    "          shuffle=True,\n",
    "          verbose=1,\n",
    "          batch_size=batch_size,\n",
    "          callbacks = callbacks_list)\n",
    "\n",
    "print('Finished training.')\n",
    "print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8760eeadb59e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model.summary() # Print a description of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plot_roc_auc(RocAuc.aucs_tra, RocAuc.aucs_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_history' is not defined"
     ]
    }
   ],
   "source": [
    "# model.summary() # Print a description of the model.\n",
    "# plot_roc_auc(RocAuc.aucs_tra, RocAuc.aucs_val)\n",
    "plot_history(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0768c83415d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y_test_pred = model.predict(test_features.tocsr(), batch_size=batch_size) #tocsr()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "del model\n",
    "gc.collect()\n",
    "# y_test_pred = model.predict(test_features.tocsr(), batch_size=batch_size) #tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperameter Tuning with Talos\n",
    "<br>\n",
    "https://autonomio.github.io/docs_talos/#lr-normalizer\n",
    "<br>\n",
    "https://github.com/autonomio/talos\n",
    "<br>\n",
    "\n",
    "Talos article:https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53\n",
    "\n",
    "Talos example:https://nbviewer.jupyter.org/github/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20with%20Keras%20for%20the%20Iris%20Prediction.ipynb\n",
    "\n",
    "Hyperas: https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\n",
    "<br>\n",
    "DeepReplay(Visualization): https://github.com/dvgodoy/deepreplay\n",
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
